### SparkSQL模块简介

1. SparkSQL是Spark的四大组件之一，也是在Spark数据处理中用得最多的组件。SparkSQL在SparkCore的基础上对外提供了SQL接口，可以让熟悉SQL的技术人员快速上手。其编程接口为SparkSession。

2. SparkSQL所有的内容位于pyspark.sql这个模块下，包含了SparkSession、Column、Row等众多的核心内容。SparkSQL是面向结构化数据的利刃，处理表格入如常规的关系型数据库一样，不管是提供的丰富的API还是执行的效率上，都有很大的优势。当然对于非结构化的数据，还得借助RDD来进行基本的处理。

3. SparkSQL中的DataFrame是一个类似于Pandas中DataFrame的表格对象，其底层是基于RDD，但是他在RDD的基础上提供了表格的例如column，row，索引等信息。因此使用基于DataFrame的API编写出的Spark程序在性能上要优于基于RDD编写出来的程序。还有一个大的优势是，迎合了很多本类就会SQL的程序员的胃口，使他们非常容易上手使用。

4. SparkSQL这个模块之前是使用的基于Hive的SQL解析器，不利于优化，随着项目的发展，Spark团队逐渐抛弃了Hive解析器，开发Spark特有的SQL解析器，使得SparkSQL在灵活性和速度上再上一层楼。在SparkSession的builder上有一个方法叫enableHiveSupport，用于启动对Hive的查询的支持。因此在使用SparkSQL的过程中，可以天然无缝的查询Hive数据仓库中的数据。

5. SparkSQL底层使用了Catalyst优化器对SQL语句进行优化，同时底层使用了Tungsten这个项目的成果，Tungsten项目聚集于对CPU和Memory的优化，借助Catalyst和Tungsten，使得SparkSQL的执行速度进一步提升，官方说法，速度为hadoop的1000倍。

6. 普通的RDD和DataFrame可以通过相关的方法进行互相转换，因此使得基于DataFrame的接口功能异常强大二灵活。SparkSQL也提供了用户自定义函数的支持，非常方便的实现了自定义的功能，在SQL中使用起来非常方便。

7. SparkSQL可以读取几乎所有格式的文件，形成DataFrame，经过业务处理之后，可以非常方便的通过write接口将结果写到文件、关系型数据库、非关系型数据库。

8. SparkSQL模块是在企业中用得最多的模块之一，而且发展迅速，值得好好研究。